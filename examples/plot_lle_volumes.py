"""
=============================================================================
Manifold learning on minc volumes: Locally Linear Embedding, Isomap...
=============================================================================

An illustration of various embeddings on the arbitrary dataset

The RandomTreesEmbedding, from the :mod:`sklearn.ensemble` module, is not
technically a manifold embedding method, as it learn a high-dimensional
representation on which we apply a dimensionality reduction method.
However, it is often useful to cast a dataset into a representation in
which the classes are linearly-separable.

t-SNE will be initialized with the embedding that is generated by PCA in
this example, which is not the default setting. It ensures global stability
of the embedding, i.e., the embedding does not depend on random
initialization.
"""

# Authors: Fabian Pedregosa <fabian.pedregosa@inria.fr>
#          Olivier Grisel <olivier.grisel@ensta.org>
#          Mathieu Blondel <mathieu@mblondel.org>
#          Gael Varoquaux
# License: BSD 3 clause (C) INRIA 2011

# Modified by: Vladimir S. FONOV <vladimir.fonov <at> gmail.com>

print(__doc__)
from time import time

import numpy as np
import scipy 
import matplotlib.pyplot as plt
from matplotlib import offsetbox
from matplotlib.backends.backend_pdf import PdfPages

from sklearn import (manifold,  decomposition, ensemble, lda,
                     random_projection)
import csv
import minc
import os

from sklearn.cluster import KMeans

class Volumes(object):
    def __init__(self, data=None, target=None):
        self.data=data
        self.target=target
        self.ids=[]
        self.thumbnails=[]
        
    def load_from_csv(self, fname):
        with open(fname,'rb') as f:
            ll=list(csv.reader(f))

        _data=[]
        _target=[]
        self.ids=[]
        self.thumbnails=[]
        
        for j,i in enumerate(ll):
            _img=minc.Image(i[0])
            _data.append(np.ravel(_img.data))
            _target.append(j)
            self.ids.append( i[1] )
            self.thumbnails.append( np.flipud(np.clip(scipy.ndimage.interpolation.zoom(_img.data[_img.data.shape[0]/2 , : ,:],0.4),0,120) ))
            
        self.data=np.vstack(tuple(_data))
        self.target=np.ravel(_target)# ravel?



volumes = Volumes()
volumes.load_from_csv('all_volumes_pscid.csv')
X = volumes.data
y = volumes.target
ids = volumes.ids
n_samples, n_features = X.shape
n_neighbors = 30
n_classes = 20

pp = PdfPages('embedding_clustering.pdf')



def find_medoids(X,cls,centroid):
    _dists=np.full(n_classes,-1.0)
    _medoids=np.full(n_classes,-1, dtype=int )
    
    for i in range(X.shape[0]):
        # calculate distance to centroid
        _c=cls[i]
        _d = np.sum( (X[i] - centroid[ _c ] ) ** 2 )
        # check if the current sample is closest
        if  _dists[_c]<0.0 or _d<_dists[_c]:
            _medoids[_c]=i
            _dists[_c]=_d
            
    return _medoids
        
#----------------------------------------------------------------------
# Scale and visualize the embedding vectors
def plot_embedding(X, cls, centroid, title=None):

    x_min, x_max = np.min(X, 0), np.max(X, 0)

    plt.figure()
    ax = plt.subplot(111)
    
    medoids=find_medoids(X, cls, centroid)
    mids=[ids[i] for i in medoids]
    print(title)
    print(mids)
    print()
    
    # normalize data range
    X = (X - x_min) / (x_max - x_min)
    centroid = (centroid- x_min) / (x_max - x_min)
 
    # display datapoints
    for i in range(X.shape[0]):
        c=plt.cm.Set1( float(cls[i]+1) / (n_classes+1) )
        plt.plot(X[i, 0], X[i, 1], '.', color=c)
        
    # displa medoids 
    if hasattr(offsetbox, 'AnnotationBbox'):
        ## only print thumbnails with matplotlib > 1.0
        for i in range(medoids.shape[0]):
            color=plt.cm.Set1( float(i+1) / (n_classes+1) )
            # display thumbnail of medioid , randomly nudging it around 
            imagebox = offsetbox.AnnotationBbox(
                offsetbox.OffsetImage(volumes.thumbnails[medoids[i]], cmap=plt.cm.gray_r),
                X[medoids[i]],
                xybox=X[medoids[i]]+[0.04,0.06]+np.random.uniform(-0.02,0.02,size=2),
                pad=0.0,
                bboxprops=dict(boxstyle="round", fc=color, ec=color,alpha=0.2),
                arrowprops=dict(arrowstyle="->") )
            
            ax.add_artist(imagebox)
            
            plt.plot(X[medoids[i], 0], X[medoids[i], 1], 'o', color=color)
            
            plt.text(X[medoids[i], 0], X[medoids[i], 1], ids[medoids[i]],
                 color=color,
                 fontdict={'weight': 'normal', 'size': 8})
    
    # show centroids
    #plt.plot(centroid[:,0], centroid[:,1], 'x', color='k')
    plt.xticks([]), plt.yticks([])
    if title is not None:
        plt.title(title)

    pp.savefig()


#----------------------------------------------------------------------
# Projection on to the first 2 principal components

print("Computing PCA projection")
t0 = time()
X_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)
cls = KMeans(init='k-means++', n_clusters=n_classes, n_init=10)
cls.fit(X_pca)
labels=cls.predict(X_pca)
centroid=cls.cluster_centers_
plot_embedding(X_pca,labels,centroid,
               "Principal Components projection of the volumes")


#----------------------------------------------------------------------
# Isomap projection of the volumes dataset
print("Computing Isomap embedding")
t0 = time()
X_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)
print("Done.")
cls = KMeans(init='k-means++', n_clusters=n_classes, n_init=10)
cls.fit(X_iso)
labels=cls.predict(X_iso)
centroid=cls.cluster_centers_
plot_embedding(X_iso,labels,centroid,
            "Isomap projection of the volumes ")


#----------------------------------------------------------------------
# Locally linear embedding of the volumes dataset
print("Computing LLE embedding")
clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,
                                    method='standard')
t0 = time()
X_lle = clf.fit_transform(X)
print("Done. Reconstruction error: %g" % clf.reconstruction_error_)
cls = KMeans(init='k-means++', n_clusters=n_classes, n_init=10)
cls.fit(X_lle)
labels=cls.predict(X_lle)
centroid=cls.cluster_centers_
plot_embedding(X_lle, labels, centroid, 
            "Locally Linear Embedding of the volumes")

#----------------------------------------------------------------------
# Modified Locally linear embedding of the volumes dataset
print("Computing modified LLE embedding")
clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,
                                    method='modified')
t0 = time()
X_mlle = clf.fit_transform(X)
cls.fit(X_mlle)
labels=cls.predict(X_mlle)
centroid=cls.cluster_centers_
print("Done. Reconstruction error: %g" % clf.reconstruction_error_)
plot_embedding(X_mlle,labels, centroid, 
            "Modified Locally Linear Embedding of the volumes ")


#----------------------------------------------------------------------
# HLLE embedding of the volumes dataset
print("Computing Hessian LLE embedding")
clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,
                                    method='hessian',eigen_solver='dense')
t0 = time()
X_hlle = clf.fit_transform(X)
print("Done. Reconstruction error: %g" % clf.reconstruction_error_)
cls = KMeans(init='k-means++', n_clusters=n_classes, n_init=10)
cls.fit(X_hlle)
labels=cls.predict(X_hlle)
centroid=cls.cluster_centers_


plot_embedding(X_hlle, labels, centroid, 
            "Hessian Locally Linear Embedding of the volumes ")


#----------------------------------------------------------------------
# LTSA embedding of the volumes dataset
#print("Computing LTSA embedding")
#clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,
                                    #method='ltsa')
#t0 = time()
#X_ltsa = clf.fit_transform(X)
#print("Done. Reconstruction error: %g" % clf.reconstruction_error_)
#cls = KMeans(init='k-means++', n_clusters=n_classes, n_init=10)
#cls.fit(X_ltsa)
#labels=cls.predict(X_ltsa)
#centroid=cls.cluster_centers_

#plot_embedding(X_ltsa,labels, centroid, 
            #"Local Tangent Space Alignment of the volumes ")

#----------------------------------------------------------------------
# MDS  embedding of the volumes dataset
print("Computing MDS embedding")
clf = manifold.MDS(n_components=2, n_init=1, max_iter=100)
t0 = time()
X_mds = clf.fit_transform(X)
print("Done. Stress: %f" % clf.stress_)

cls = KMeans(init='k-means++', n_clusters=n_classes, n_init=10)
cls.fit(X_mds)
labels=cls.predict(X_mds)
centroid=cls.cluster_centers_

plot_embedding(X_mds,labels, centroid, 
            "MDS embedding of the volumes ")

#----------------------------------------------------------------------
# Random Trees embedding of the volumes dataset
print("Computing Totally Random Trees embedding")
hasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0,
                                    max_depth=5)
t0 = time()
X_transformed = hasher.fit_transform(X)
pca = decomposition.TruncatedSVD(n_components=2)
X_reduced = pca.fit_transform(X_transformed)

plot_embedding(X_reduced,labels, centroid, 
            "Random forest embedding of the volumes ")

#----------------------------------------------------------------------
# Spectral embedding of the volumes dataset
print("Computing Spectral embedding")
embedder = manifold.SpectralEmbedding(n_components=2, random_state=0,
                                    eigen_solver="arpack")
t0 = time()
X_se = embedder.fit_transform(X)

cls = KMeans(init='k-means++', n_clusters=n_classes, n_init=10)
cls.fit(X_se)
labels=cls.predict(X_se)
centroid=cls.cluster_centers_

plot_embedding(X_se, labels, centroid, 
            "Spectral embedding of the volumes ")

#----------------------------------------------------------------------
# t-SNE embedding of the volumes dataset
print("Computing t-SNE embedding")
tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)
t0 = time()
X_tsne = tsne.fit_transform(X)

cls = KMeans(init='k-means++', n_clusters=n_classes, n_init=10)
cls.fit(X_tsne)
labels=cls.predict(X_tsne)
centroid=cls.cluster_centers_

plot_embedding(X_tsne, labels, centroid, 
            "t-SNE embedding of the volumes ")

pp.close()

#plt.show()

# kate: space-indent on; indent-width 4; indent-mode python;replace-tabs on;word-wrap-column 80
